{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T02:01:46.858681500Z",
     "start_time": "2024-07-11T02:01:43.673168600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ],
   "id": "6c744b0e015386e6"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T02:01:46.897681400Z",
     "start_time": "2024-07-11T02:01:46.877684600Z"
    }
   },
   "outputs": [],
   "source": [
    "def smooth_ts(original_data, window_len):\n",
    "    smoothed_df = pd.DataFrame()\n",
    "    for i, n in enumerate(np.unique(original_data['unit_number'])):\n",
    "        temp_df = original_data[original_data['unit_number'] == n].copy()\n",
    "        temp_df.drop(columns=['unit_number', 'time_in_cycles', 'RUL'], inplace=True)\n",
    "        temp_df = temp_df.rolling(window_len).mean()\n",
    "        smoothed_df = pd.concat([smoothed_df, temp_df], axis=0)\n",
    "    smoothed_df.rename(columns={name: name + '_mv_avg' for name in smoothed_df.columns}, inplace=True)\n",
    "    data = pd.concat([original_data, smoothed_df], axis=1)\n",
    "    #data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_rsi(original_data, cols, window_len=21):\n",
    "    rsi = pd.DataFrame()\n",
    "    for i, n in enumerate(np.unique(original_data['unit_number'])):\n",
    "        init_data = original_data[original_data['unit_number'] == n].copy()\n",
    "        init_data = init_data[cols]\n",
    "        init_data.drop(columns=['unit_number', 'time_in_cycles', 'RUL'], inplace=True)        \n",
    "        delta = init_data.diff()\n",
    "        up, down = delta.clip(lower=0), delta.clip(upper=0).abs()\n",
    "        roll_up = up.rolling(window=window_len).sum() / window_len\n",
    "        roll_down = down.rolling(window=window_len).sum().abs() / window_len\n",
    "        rs = roll_up / roll_down\n",
    "        rsi_temp = 100 / (1 + rs)\n",
    "        if i ==0:\n",
    "            rsi = rsi_temp\n",
    "        else:\n",
    "            rsi = pd.concat((rsi,rsi_temp),axis =0)\n",
    "        \n",
    "        \n",
    "    rsi.rename(columns={name: name + '_rsi' for name in rsi.columns}, inplace=True)\n",
    "    data = pd.concat([original_data, rsi], axis=1)\n",
    "    #data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def add_BB(original_data, cols):\n",
    "    bb = pd.DataFrame()\n",
    "    for i, n in enumerate(np.unique(original_data['unit_number'])):\n",
    "        init_data = original_data[original_data['unit_number'] == n].copy()\n",
    "        #init_data = init_data[cols]\n",
    "        init_data.drop(columns=['unit_number', 'time_in_cycles', 'RUL'], inplace=True)\n",
    "        sma = init_data.rolling(window=20).mean()\n",
    "        ub = sma + 2 * init_data.rolling(window=20).std()\n",
    "        lb = sma - 2 * init_data.rolling(window=20).std()\n",
    "        pbb = (init_data - lb) / (ub - lb)\n",
    "        if i ==0:\n",
    "            bb = pbb\n",
    "        else:\n",
    "            bb = pd.concat((bb,pbb),axis =0)\n",
    "        \n",
    "        #bb = pd.concat([bb, pbb], axis=0)\n",
    "    bb.rename(columns={name: name + '_pbb' for name in bb.columns}, inplace=True)\n",
    "    data = pd.concat([original_data, bb], axis=1)\n",
    "    #data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_CCI(original_data, ndays=20):\n",
    "    cci_list = []\n",
    "    for i, n in enumerate(np.unique(original_data['unit_number'])):\n",
    "        init_data = original_data[original_data['unit_number'] == n].copy()\n",
    "        init_data['high'] = init_data.max(axis=1)\n",
    "        init_data['low'] = init_data.min(axis=1)\n",
    "        init_data['close'] = init_data.mean(axis=1)\n",
    "        TP = (init_data['high'] + init_data['low'] + init_data['close']) / 3\n",
    "        CCI = (TP - TP.rolling(ndays).mean()) / (0.015 * TP.rolling(ndays).std())\n",
    "        cci_list.append(CCI)\n",
    "\n",
    "    cci = pd.concat(cci_list, axis=0)\n",
    "    cci.name = 'CCI'\n",
    "    cci_df = cci.to_frame()\n",
    "    cci_df.rename(columns={'CCI': 'CCI_cci'}, inplace=True)\n",
    "    \n",
    "    data = pd.concat([original_data, cci_df], axis=1)\n",
    "    #data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def add_stochastic_oscillator(original_data, window_len=20):\n",
    "    stoch_k_list = []\n",
    "    stoch_d_list = []\n",
    "    \n",
    "    for n in np.unique(original_data['unit_number']):\n",
    "        init_data = original_data[original_data['unit_number'] == n].copy()\n",
    "        init_data['high'] = init_data.max(axis=1)\n",
    "        init_data['low'] = init_data.min(axis=1)\n",
    "        init_data['close'] = init_data.mean(axis=1)\n",
    "        \n",
    "        L14 = init_data['low'].rolling(window=window_len).min()\n",
    "        H14 = init_data['high'].rolling(window=window_len).max()\n",
    "        stoch_k_temp = 100 * ((init_data['close'] - L14) / (H14 - L14))\n",
    "        stoch_d_temp = stoch_k_temp.rolling(window=3).mean()  # Smoothed %K to get %D\n",
    "        \n",
    "        stoch_k_list.append(stoch_k_temp)\n",
    "        stoch_d_list.append(stoch_d_temp)\n",
    "    \n",
    "    stoch_k = pd.concat(stoch_k_list, axis=0)\n",
    "    stoch_d = pd.concat(stoch_d_list, axis=0)\n",
    "    \n",
    "    stoch_k.name = 'stoch_k'\n",
    "    stoch_d.name = 'stoch_d'\n",
    "    \n",
    "    data = pd.concat([original_data, stoch_k, stoch_d], axis=1)\n",
    "    #data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def add_rolling_std(original_data, cols, window_len = 14):\n",
    "    bb = pd.DataFrame()\n",
    "    for i, n in enumerate(np.unique(original_data['unit_number'])):\n",
    "        init_data = original_data[original_data['unit_number'] == n].copy()\n",
    "        init_data = init_data[cols]\n",
    "        init_data.drop(columns=['unit_number', 'time_in_cycles', 'RUL'], inplace=True)\n",
    "        stdev = init_data.rolling(window=window_len).std()\n",
    "        if i ==0:\n",
    "            rstdev = stdev\n",
    "        else:\n",
    "            rstdev = pd.concat((rstdev,stdev),axis =0)\n",
    "    rstdev.rename(columns={name: name + '_std' for name in rstdev.columns}, inplace=True)\n",
    "    data = pd.concat([original_data, rstdev], axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def calculate_score(estimated_rul, true_rul, a1=10, a2=13):\n",
    "    score = []\n",
    "    for i in range(len(true_rul)):\n",
    "        d = estimated_rul[i] - true_rul[i]\n",
    "        if d < 0:\n",
    "            score.append(np.exp(-d / a1) - 1)\n",
    "        else:\n",
    "            score.append(np.exp(d / a2) - 1)\n",
    "    return np.sum(score)"
   ],
   "id": "9b5939281015a5b9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T02:01:50.248683Z",
     "start_time": "2024-07-11T02:01:46.899684100Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_FD001.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 13\u001B[0m\n\u001B[0;32m      9\u001B[0m headers \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munit_number\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_in_cycles\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msetting_1\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msetting_2\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msetting_3\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT2\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT24\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT30\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT50\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP2\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP15\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP30\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNf\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     10\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNc\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepr\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPs30\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphi\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNRf\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNRc\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBPR\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfarB\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtBleed\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNf_dmd\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPCNfR_dmd\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mW31\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mW32\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Read the training & test data\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_file_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelim_whitespace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m test_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(test_file_path, delim_whitespace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, names\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[0;32m     15\u001B[0m rul_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(rul_file_path, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, names\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRUL\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'train_FD001.txt'"
     ]
    }
   ],
   "source": [
    "############### Data Pre-Processing ##########################################################################   \n",
    "\n",
    "data_set = 1      #specify which dataset to read\n",
    "train_file_path = 'train_FD00' + str(data_set) + '.txt'\n",
    "test_file_path = 'test_FD00' + str(data_set) + '.txt'\n",
    "rul_file_path = 'RUL_FD00' + str(data_set) + '.txt'\n",
    "\n",
    "\n",
    "headers = ['unit_number','time_in_cycles','setting_1','setting_2','setting_3','T2','T24','T30','T50','P2','P15','P30','Nf',\n",
    "           'Nc','epr','Ps30','phi','NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "# Read the training & test data\n",
    "train_data = pd.read_csv(train_file_path, delim_whitespace=True, header=None, names=headers)\n",
    "test_data = pd.read_csv(test_file_path, delim_whitespace=True, header=None, names=headers)\n",
    "rul_data = pd.read_csv(rul_file_path, header=None, names=['RUL'])\n",
    "max_cycle_train = train_data.groupby('unit_number')['time_in_cycles'].max().reset_index()# Calculate the maximum cycle number for each unit in the training data\n",
    "max_cycle_train.columns = ['unit_number', 'max_cycle']\n",
    "train_data = train_data.merge(max_cycle_train, on='unit_number', how='left')# Merge the max_cycle_train dataframe with the train data to calculate the RUL for each entry\n",
    "train_data['RUL'] = train_data['max_cycle'] - train_data['time_in_cycles']# Calculate the RUL for each entry in the train data\n",
    "train_data = train_data.drop(columns=['max_cycle'])# Drop the temporary max_cycle column\n",
    "rul_data.columns = ['RUL']\n",
    "rul_data['unit_number'] = rul_data.index + 1\n",
    "max_cycle_test = test_data.groupby('unit_number')['time_in_cycles'].max().reset_index()# Calculate the maximum cycle number for each unit in the test data\n",
    "max_cycle_test.columns = ['unit_number', 'max_cycle']\n",
    "test_data = test_data.merge(max_cycle_test, on='unit_number', how='left')# Merge the max_cycle_test dataframe with the test data to calculate the RUL for each entry\n",
    "\n",
    "# Assign the RUL values to the test data based on unit number\n",
    "# The provided RUL is at the end of the data collection, add this RUL to the difference between max cycle and current cycle\n",
    "test_data['RUL'] = test_data.apply(lambda row: rul_data.loc[rul_data['unit_number'] == row['unit_number'], 'RUL'].values[0] + (row['max_cycle'] - row['time_in_cycles']), axis=1)\n",
    "test_data = test_data.drop(columns=['max_cycle'])# Drop the temporary max_cycle column\n",
    "\n",
    "\n",
    "######### Calculate VIF to determine which variables to keep  \n",
    "vif_thresh = 10\n",
    "features = train_data.drop(columns=['unit_number', 'time_in_cycles', 'RUL'])# Drop unnecessary columns\n",
    "vif_data = pd.DataFrame()\n",
    "scaler = StandardScaler()  # Standardize the data\n",
    "x = scaler.fit_transform(features)\n",
    "standardized_data = pd.DataFrame(x, columns=features.columns) # Create a DataFrame for the standardized data\n",
    "var_df = features.var()\n",
    "#non_zero_variance_features = standardized_data.loc[:, standardized_data.var() != 0] # Remove features with zero variance\n",
    "non_zero_variance_features = standardized_data.loc[:, features.var() > 0.001]\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame() \n",
    "vif_data['Feature'] = non_zero_variance_features.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(non_zero_variance_features.values, i) for i in range(non_zero_variance_features.shape[1])]\n",
    "vif_data['Feature'] = non_zero_variance_features.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(non_zero_variance_features.values, i) for i in range(non_zero_variance_features.shape[1])]\n",
    "\n",
    "if data_set in [2,4]:\n",
    "    keep = ['unit_number', 'time_in_cycles','RUL']+list(vif_data['Feature'][vif_data['VIF']<= vif_thresh])\n",
    "    include= ['Nf','Nc','NRf','NRc','T24','T30','P30','W32','phi','BPR','htBleed']\n",
    "    for i in include:\n",
    "        if not i in keep:\n",
    "            keep.append(i)\n",
    "else:\n",
    "    keep = ['unit_number','time_in_cycles','setting_1','T24','T30','T50','P30','Nf','Nc','Ps30','phi','NRf','NRc','BPR','htBleed','W31','W32','RUL']\n",
    "\n",
    "############### Create training and test sets with added time series/ indicator variables ######################\n",
    "train_data = train_data[keep]\n",
    "test_data = test_data[keep]\n",
    "\n",
    "# Add RSI values\n",
    "train_data = add_rsi(train_data,cols=keep, window_len=21)\n",
    "test_data = add_rsi(test_data,cols=keep, window_len=21)\n",
    "\n",
    "# Add BB values\n",
    "train_data = add_BB(train_data,keep)\n",
    "test_data = add_BB(test_data,keep)\n",
    "\n",
    "# #Add CCI values\n",
    "train_data = add_CCI(train_data, ndays=20)\n",
    "test_data = add_CCI(test_data, ndays=20)\n",
    "\n",
    "# Add Stochastic Oscillator values\n",
    "train_data = add_stochastic_oscillator(train_data, window_len=14)\n",
    "test_data = add_stochastic_oscillator(test_data,  window_len=14)\n",
    "\n",
    "\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Prepare the data for training\n",
    "X_train = train_data.drop(columns=['unit_number', 'time_in_cycles', 'RUL'])\n",
    "y_train = train_data[['unit_number', 'time_in_cycles', 'RUL']]\n",
    "X_test = test_data.drop(columns=['unit_number', 'time_in_cycles', 'RUL'])\n",
    "y_test = test_data[['unit_number', 'time_in_cycles', 'RUL']]"
   ],
   "id": "4b1d147bc23f086c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T02:01:49.691684300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the data (assuming you have the data loaded in X_train, y_train, and test_data)\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Filter test data to get the rows with the maximum time_in_cycles for each unit_number\n",
    "max_cycles = test_data.groupby('unit_number')['time_in_cycles'].max().reset_index()\n",
    "test_max_cycles = pd.merge(test_data, max_cycles, on=['unit_number', 'time_in_cycles'], how='inner')\n",
    "\n",
    "# Scale the test data\n",
    "X_test_max_scaled = scaler.transform(test_max_cycles.drop(columns=['unit_number', 'time_in_cycles', 'RUL']))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer for regression task\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train['RUL'], epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test_max_scaled, test_max_cycles['RUL'])\n",
    "print(f'Test Mean Absolute Error: {mae:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_max_scaled)\n",
    "predictions[predictions <0] = 0\n",
    "\n",
    "# Calculate the R² value\n",
    "r2 = r2_score(test_max_cycles['RUL'], predictions)\n",
    "print(f'R² Value: {r2:.4f}')\n",
    "\n",
    "# Calculate the score using the custom score function\n",
    "model_score = calculate_score(predictions.flatten(), test_max_cycles['RUL'].values)\n",
    "\n",
    "print(f'Model Score: {model_score:.4f}')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_max_cycles['unit_number'], test_max_cycles['RUL'], label='True RUL', marker='o')\n",
    "plt.plot(test_max_cycles['unit_number'], predictions, label='Predicted RUL', marker='x')\n",
    "plt.title('FD003 True vs Predicted RUL')\n",
    "plt.xlabel('Unit Number')\n",
    "plt.ylabel('RUL')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "3117ea621da53940"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import h5py"
   ],
   "id": "1428ab91a84f3a6"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.save']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model using the recommended Keras format\n",
    "model.save('trained_model.keras')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.save')"
   ],
   "id": "75f2eea33d56e84a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m190/190\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 629us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model('trained_model.keras')\n",
    " \n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.save')\n",
    " \n",
    "# Assuming X_test is your test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predictions = model.predict(X_test_scaled)"
   ],
   "id": "6ff7a734e6cace63"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\baris\\\\Practicum\\\\model_and_scaler.zip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive('model_and_scaler', 'zip', '.', 'trained_model.keras')\n",
    "shutil.make_archive('model_and_scaler', 'zip', '.', 'scaler.save')"
   ],
   "id": "7457679d529d0e04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "id": "824de9930bda0e7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
